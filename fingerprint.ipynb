{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fingerprint.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1uw4dH2wKWmqX7Bic_TDtsPDiAB-ZNDRl",
      "authorship_tag": "ABX9TyO5uCzW7uD/YGKC39noYHXY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utsav-195/fingerprint-recognition-using-siamese-network/blob/master/fingerprint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrqyAzxChL2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQiznhCQhp4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing the required libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import imageio\n",
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa\n",
        "\n",
        "from tensorflow import keras\n",
        "from PIL import Image"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4sU-MHbCo1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def augment(folder,file):\n",
        "  filename = folder + \"/\" + file\n",
        "  # loading in the images\n",
        "  image = imageio.imread(filename)\n",
        "\n",
        "  flip_vr=iaa.Flipud(p=1.0)\n",
        "  flip_vr_image= flip_vr.augment_image(image)\n",
        "\n",
        "  save_filename = folder + \"/\" + file.split(\".\")[0] + \"_flipped.png\"\n",
        "  cv2.imwrite(save_filename, flip_vr_image)\n",
        "\n",
        "  rotate = iaa.Affine(rotate=(50, -50))\n",
        "  rotated_image = rotate.augment_image(image)\n",
        "  save_filename = folder + \"/\" + file.split(\".\")[0] + \"_rotated1.png\"\n",
        "  cv2.imwrite(save_filename, rotated_image)\n",
        "\n",
        "  rotate = iaa.Affine(rotate=(50, -50))\n",
        "  rotated_image = rotate.augment_image(image)\n",
        "  save_filename = folder + \"/\" + file.split(\".\")[0] + \"_rotated2.png\"\n",
        "  cv2.imwrite(save_filename, rotated_image)\n",
        "\n",
        "  crop = iaa.Crop(percent=(0, 0.3)) # crop image\n",
        "  crop_image=crop.augment_image(image)\n",
        "  save_filename = folder + \"/\" + file.split(\".\")[0] + \"_cropped1.png\"\n",
        "  cv2.imwrite(save_filename, crop_image)\n",
        "\n",
        "  crop = iaa.Crop(percent=(0, 0.3)) # crop image\n",
        "  crop_image=crop.augment_image(image)\n",
        "  save_filename = folder + \"/\" + file.split(\".\")[0] + \"_cropped2.png\"\n",
        "  cv2.imwrite(save_filename, crop_image)\n",
        "\n",
        "  contrast=iaa.GammaContrast(gamma=2.0)\n",
        "  contrast_image =contrast.augment_image(image)\n",
        "  save_filename = folder + \"/\" + file.split(\".\")[0] + \"_bright1.png\"\n",
        "  cv2.imwrite(save_filename, contrast_image)\n",
        "\n",
        "  contrast=iaa.GammaContrast(gamma=1.4)\n",
        "  contrast_image =contrast.augment_image(image)\n",
        "  save_filename = folder + \"/\" + file.split(\".\")[0] + \"_bright2.png\"\n",
        "  cv2.imwrite(save_filename, contrast_image)\n",
        "\n",
        "  blur = iaa.GaussianBlur(sigma=4.0)\n",
        "  blur_image=blur.augment_image(image)\n",
        "  save_filename = folder + \"/\" + file.split(\".\")[0] + \"_blur1.png\"\n",
        "  cv2.imwrite(save_filename, blur_image)\n",
        "\n",
        "  blur = iaa.GaussianBlur(sigma=2.0)\n",
        "  blur_image=blur.augment_image(image)\n",
        "  save_filename = folder + \"/\" + file.split(\".\")[0] + \"_blur2.png\"\n",
        "  cv2.imwrite(save_filename, blur_image)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwtNBh1rjOsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dimen = 224\n",
        "\n",
        "dir_path = \"/content/drive/My Drive/fingerprint/images/\"\n",
        "out_path = \"/content/drive/My Drive/fingerprint/processed_images/\"\n",
        "model_path = \"/content/drive/My Drive/fingerprint/model/\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCpqIaxJ_zF7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d534abcb-07ed-49f3-f50a-e1115925e1ec"
      },
      "source": [
        "sub_dir_list = os.listdir(dir_path)\n",
        "\n",
        "images = []\n",
        "for i in range(len(sub_dir_list)):\n",
        "  # image_names = os.listdir(os.path.join(dir_path, sub_dir_list[i]))\n",
        "  # augment(os.path.join(dir_path, sub_dir_list[i]),image_names[0])\n",
        "  image_names = os.listdir(os.path.join(dir_path, sub_dir_list[i]))\n",
        "  sub_dir_images = []\n",
        "  for image_path in image_names:\n",
        "    path = os.path.join(dir_path, sub_dir_list[i], image_path )\n",
        "    try:\n",
        "      print(path)\n",
        "      image = Image.open(path)\n",
        "      resize_image = image.resize((dimen, dimen))\n",
        "      array_ = list()\n",
        "      for x in range(dimen):\n",
        "        sub_array = list()\n",
        "        for y in range(dimen):\n",
        "          sub_array.append(resize_image.load()[x, y])\n",
        "        array_.append(sub_array)\n",
        "      image_data = np.array(array_)\n",
        "      image = np.array(np.reshape(image_data, (dimen, dimen, 3))) / 255\n",
        "      sub_dir_images.append(image)\n",
        "      images.append(image)\n",
        "    except:\n",
        "      print('WARNING : File {} could not be processed.'.format(path))\n",
        "  sub_dir_images = np.array(sub_dir_images)\n",
        "  np.save( '{0}/{1}_processed.npy'.format(os.path.join(dir_path, sub_dir_list[i]),str(sub_dir_list[i])), sub_dir_images )\n",
        "  print(\"Save Complete\")\n",
        "\n",
        "images = np.array(images)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/fingerprint/images/thumb/thumb.jpg\n",
            "/content/drive/My Drive/fingerprint/images/thumb/thumb_flipped.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb/thumb_rotated1.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb/thumb_rotated2.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb/thumb_cropped1.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb/thumb_cropped2.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb/thumb_bright1.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb/thumb_bright2.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb/thumb_blur1.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb/thumb_blur2.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb/thumb_processed.npy\n",
            "WARNING : File /content/drive/My Drive/fingerprint/images/thumb/thumb_processed.npy could not be processed.\n",
            "Save Complete\n",
            "/content/drive/My Drive/fingerprint/images/first/first.jpg\n",
            "/content/drive/My Drive/fingerprint/images/first/first_flipped.png\n",
            "/content/drive/My Drive/fingerprint/images/first/first_rotated1.png\n",
            "/content/drive/My Drive/fingerprint/images/first/first_rotated2.png\n",
            "/content/drive/My Drive/fingerprint/images/first/first_cropped1.png\n",
            "/content/drive/My Drive/fingerprint/images/first/first_cropped2.png\n",
            "/content/drive/My Drive/fingerprint/images/first/first_bright1.png\n",
            "/content/drive/My Drive/fingerprint/images/first/first_bright2.png\n",
            "/content/drive/My Drive/fingerprint/images/first/first_blur1.png\n",
            "/content/drive/My Drive/fingerprint/images/first/first_blur2.png\n",
            "/content/drive/My Drive/fingerprint/images/first/first_processed.npy\n",
            "WARNING : File /content/drive/My Drive/fingerprint/images/first/first_processed.npy could not be processed.\n",
            "Save Complete\n",
            "/content/drive/My Drive/fingerprint/images/middle/middle.jpg\n",
            "/content/drive/My Drive/fingerprint/images/middle/middle_flipped.png\n",
            "/content/drive/My Drive/fingerprint/images/middle/middle_rotated1.png\n",
            "/content/drive/My Drive/fingerprint/images/middle/middle_rotated2.png\n",
            "/content/drive/My Drive/fingerprint/images/middle/middle_cropped1.png\n",
            "/content/drive/My Drive/fingerprint/images/middle/middle_cropped2.png\n",
            "/content/drive/My Drive/fingerprint/images/middle/middle_bright1.png\n",
            "/content/drive/My Drive/fingerprint/images/middle/middle_bright2.png\n",
            "/content/drive/My Drive/fingerprint/images/middle/middle_blur1.png\n",
            "/content/drive/My Drive/fingerprint/images/middle/middle_blur2.png\n",
            "/content/drive/My Drive/fingerprint/images/middle/middle_processed.npy\n",
            "WARNING : File /content/drive/My Drive/fingerprint/images/middle/middle_processed.npy could not be processed.\n",
            "Save Complete\n",
            "/content/drive/My Drive/fingerprint/images/ring/ring.jpg\n",
            "/content/drive/My Drive/fingerprint/images/ring/ring_flipped.png\n",
            "/content/drive/My Drive/fingerprint/images/ring/ring_rotated1.png\n",
            "/content/drive/My Drive/fingerprint/images/ring/ring_rotated2.png\n",
            "/content/drive/My Drive/fingerprint/images/ring/ring_cropped1.png\n",
            "/content/drive/My Drive/fingerprint/images/ring/ring_cropped2.png\n",
            "/content/drive/My Drive/fingerprint/images/ring/ring_bright1.png\n",
            "/content/drive/My Drive/fingerprint/images/ring/ring_bright2.png\n",
            "/content/drive/My Drive/fingerprint/images/ring/ring_blur1.png\n",
            "/content/drive/My Drive/fingerprint/images/ring/ring_blur2.png\n",
            "/content/drive/My Drive/fingerprint/images/ring/ring_processed.npy\n",
            "WARNING : File /content/drive/My Drive/fingerprint/images/ring/ring_processed.npy could not be processed.\n",
            "Save Complete\n",
            "/content/drive/My Drive/fingerprint/images/pinky/pinky.jpg\n",
            "/content/drive/My Drive/fingerprint/images/pinky/pinky_flipped.png\n",
            "/content/drive/My Drive/fingerprint/images/pinky/pinky_rotated1.png\n",
            "/content/drive/My Drive/fingerprint/images/pinky/pinky_rotated2.png\n",
            "/content/drive/My Drive/fingerprint/images/pinky/pinky_cropped1.png\n",
            "/content/drive/My Drive/fingerprint/images/pinky/pinky_cropped2.png\n",
            "/content/drive/My Drive/fingerprint/images/pinky/pinky_bright1.png\n",
            "/content/drive/My Drive/fingerprint/images/pinky/pinky_bright2.png\n",
            "/content/drive/My Drive/fingerprint/images/pinky/pinky_blur1.png\n",
            "/content/drive/My Drive/fingerprint/images/pinky/pinky_blur2.png\n",
            "/content/drive/My Drive/fingerprint/images/pinky/pinky_processed.npy\n",
            "WARNING : File /content/drive/My Drive/fingerprint/images/pinky/pinky_processed.npy could not be processed.\n",
            "Save Complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnU8l6rmf6WL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samples_1 = []\n",
        "samples_2 = []\n",
        "labels = []\n",
        "\n",
        "for i in range(len(images)):\n",
        "  for j in range(len(images)):\n",
        "    samples_1.append(images[i])\n",
        "    samples_2.append(images[j])\n",
        "    t = i - i%10 +10\n",
        "    if t - 10 <= j < t:\n",
        "      labels.append(1)\n",
        "    else:\n",
        "      labels.append(0)\n",
        "\n",
        "X1 = np.array(samples_1)\n",
        "X2 = np.array(samples_2)\n",
        "Y = np.array(labels)\n",
        "\n",
        "np.save( '{}/images.npy'.format( out_path ), images )\n",
        "np.save( '{}/x1.npy'.format( out_path ), X1 )\n",
        "np.save( '{}/x2.npy'.format( out_path ), X2 )\n",
        "np.save( '{}/y.npy'.format( out_path ) , Y )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iz-D8i3AFIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import models , optimizers , losses ,activations , callbacks\n",
        "from tensorflow.keras.layers import *\n",
        "import tensorflow.keras.backend as K\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "class Recognizer (object) :\n",
        "\n",
        "\tdef __init__( self ):\n",
        "\n",
        "\t\t# tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "\t\tself.__DIMEN = 224\n",
        "\n",
        "\t\tinput_shape = ((self.__DIMEN**2) * 3 , )\n",
        "\t\tconvolution_shape = (self.__DIMEN , self.__DIMEN , 3)\n",
        "\t\tkernel_size_1 = (10 ,10)\n",
        "\t\tkernel_size_2 = (7 ,7)\n",
        "\t\tkernel_size_3 = (4 ,4)\n",
        "\t\t# kernel_size_2 = ( 7 , 7 )\n",
        "\t\n",
        "\t\t# pool_size_1 = ( 3 , 3 )\n",
        "\t\t# pool_size_2 = ( 2 , 2 )\n",
        "\t\tstrides = 1\n",
        "\n",
        "\t\tseq_conv_model = [\n",
        "\n",
        "\t\t\tReshape( input_shape=input_shape , target_shape=convolution_shape),\n",
        "\n",
        "\t\t\tConv2D( 64, kernel_size=kernel_size_1 , strides=strides , activation='relu' ),\n",
        "\t\t\t# Conv2D( 32, kernel_size=kernel_size_1, strides=strides, activation='relu'),\n",
        "\t\t\tMaxPooling2D(),\n",
        "\n",
        "\t\t\tConv2D( 128, kernel_size=kernel_size_2 , strides=strides , activation='relu'),\n",
        "\t\t\t# Conv2D( 64, kernel_size=kernel_size_2 , strides=strides , activation='relu'),\n",
        "\t\t\tMaxPooling2D(),\n",
        "\n",
        "      Conv2D( 128, kernel_size=kernel_size_3 , strides=strides , activation='relu'),\n",
        "\t\t\t# Conv2D( 128, kernel_size=kernel_size_3 , strides=strides , activation='relu'),\n",
        "\t\t\tMaxPooling2D(),\n",
        "\n",
        "\t\t\tConv2D( 256, kernel_size=kernel_size_3 , strides=strides , activation='relu'),\n",
        "\t\t\t# Conv2D( 128, kernel_size=kernel_size_3 , strides=strides , activation='relu'),\n",
        "\n",
        "\t\t\tFlatten(),\n",
        "\n",
        "\t\t\tDense(1024, activation='relu'),\n",
        "\t \t\tDense(1024, activation=activations.sigmoid)\n",
        "\n",
        "\t\t]\n",
        "\n",
        "\t\tseq_model = tf.keras.Sequential(seq_conv_model)\n",
        "\n",
        "\t\tinput_x1 = Input(shape=input_shape)\n",
        "\t\tinput_x2 = Input(shape=input_shape)\n",
        "\n",
        "\t\toutput_x1 = seq_model(input_x1)\n",
        "\t\toutput_x2 = seq_model(input_x2)\n",
        "\n",
        "\t\tdistance_euclid = Lambda( lambda tensors : K.abs( tensors[0] - tensors[1] ))( [output_x1 , output_x2] )\n",
        "\t\toutputs = Dense( 1 , activation=activations.sigmoid) ( distance_euclid )\n",
        "\t\tself.__model = models.Model( [ input_x1 , input_x2 ] , outputs )\n",
        "\n",
        "\t\tself.__model.compile( loss=losses.binary_crossentropy , optimizer=optimizers.Adam(lr=0.00003))\n",
        "\n",
        "\n",
        "\n",
        "\tdef fit(self, X, Y, hyperparameters):\n",
        "\t\tinitial_time = time.time()\n",
        "\t\tself.__model.fit( X, Y,\n",
        "\t\t\t\t\t\t batch_size=hyperparameters[ 'batch_size' ] ,\n",
        "\t\t\t\t\t\t epochs=hyperparameters[ 'epochs' ] ,\n",
        "\t\t\t\t\t\t callbacks=hyperparameters[ 'callbacks'],\n",
        "\t\t\t\t\t\t validation_data=hyperparameters[ 'val_data' ]\n",
        "\t\t\t\t\t\t )\n",
        "\t\tfinal_time = time.time()\n",
        "\t\teta = (final_time - initial_time)\n",
        "\t\ttime_unit = 'seconds'\n",
        "\t\tif eta >= 60 :\n",
        "\t\t\teta = eta / 60\n",
        "\t\t\ttime_unit = 'minutes'\n",
        "\t\tself.__model.summary( )\n",
        "\t\tprint( 'Elapsed time acquired for {} epoch(s) -> {} {}'.format( hyperparameters[ 'epochs' ] , eta , time_unit ) )\n",
        "\n",
        "\n",
        "\tdef prepare_images_from_dir( self , dir_path , flatten=True ):\n",
        "\t\timages_names = os.listdir(dir_path)\n",
        "\t\tf = [name for name in images_names if '.npy' in name]\n",
        "\t\tif len(f) == 0:\n",
        "\t\t\timages = list()\n",
        "\t\t\tfor imageName in images_names:\n",
        "\t\t\t\t# print(imageName)\n",
        "\t\t\t\timage = Image.open(dir_path + imageName)\n",
        "\t\t\t\tresize_image = image.resize((self.__DIMEN, self.__DIMEN))\n",
        "\t\t\t\tarray = list()\n",
        "\t\t\t\tfor x in range(self.__DIMEN):\n",
        "\t\t\t\t\tsub_array = list()\n",
        "\t\t\t\t\tfor y in range(self.__DIMEN):\n",
        "\t\t\t\t\t\tsub_array.append(resize_image.load()[x, y])\n",
        "\t\t\t\t\tarray.append(sub_array)\n",
        "\t\t\t\timage_data = np.array(array)\n",
        "\t\t\t\timage = np.array(np.reshape(image_data,(self.__DIMEN, self.__DIMEN, 3))) /255\n",
        "\t\t\t\timages.append(image)\n",
        "\n",
        "\t\t\tif flatten:\n",
        "\t\t\t\timages = np.array(images)\n",
        "\t\t\t\treturn images.reshape(images.shape[0], self.__DIMEN**2 * 3)).astype(np.float32)\n",
        "\t\t\telse:\n",
        "\t\t\t\treturn np.array(images)\n",
        "\t\telse:\n",
        "\t\t\timages = np.load('{0}{1}'.format(dir_path,f[0]))\n",
        "\t\t\tif flatten:\n",
        "\t\t\t\timages = np.array(images)\n",
        "\t\t\t\treturn images.reshape((images.shape[0], self.__DIMEN**2 * 3)).astype(np.float32)\n",
        "\t\t\telse:\n",
        "\t\t\t\treturn np.array(images)\n",
        "\n",
        "\n",
        "\tdef evaluate(self, test_X, test_Y) :\n",
        "\t\treturn self.__model.evaluate(test_X, test_Y)\n",
        "\n",
        "\n",
        "\tdef predict(self, X):\n",
        "\t\tpredictions = self.__model.predict(X)\n",
        "\t\treturn predictions\n",
        "\n",
        "\n",
        "\tdef summary(self):\n",
        "\t\tself.__model.summary()\n",
        "\n",
        "\tdef save_model(self, file_path):\n",
        "\t\tself.__model.save(file_path)\n",
        "\n",
        "\n",
        "\tdef load_model(self, file_path):\n",
        "\t\tself.__model = models.load_model(file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsDtTHOXAKeX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "13ab7dc7-6d13-4261-f86a-945a2caa29a5"
      },
      "source": [
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "data_dimension = 224\n",
        "\n",
        "X1 = np.load('{}/x1.npy'.format(out_path))\n",
        "X2 = np.load('{}/x2.npy'.format(out_path))\n",
        "Y = np.load('{}/y.npy'.format(out_path))\n",
        "\n",
        "X1 = X1.reshape((X1.shape[0], data_dimension**2 * 3)).astype(np.float32)\n",
        "X2 = X2.reshape((X2.shape[0], data_dimension**2 * 3)).astype(np.float32)\n",
        "\n",
        "print(X1.shape)\n",
        "print(X2.shape)\n",
        "print(Y.shape)\n",
        "\n",
        "recognizer = Recognizer()\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# checkpointer to save the best model\n",
        "checkpointer = ModelCheckpoint(filepath='{}/model_test5.h5'.format(model_path), monitor='loss', verbose=1, save_best_only=True)\n",
        "\n",
        "parameters = {\n",
        "    'batch_size' : 50,\n",
        "    'epochs' : 30,\n",
        "    'callbacks' : checkpointer,\n",
        "    'val_data' : None\n",
        "}\n",
        "\n",
        "recognizer.fit( [ X1 , X2 ], Y, hyperparameters=parameters)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2500, 150528)\n",
            "(2500, 150528)\n",
            "(2500,)\n",
            "Epoch 1/20\n",
            " 2/50 [>.............................] - ETA: 12s - loss: 0.6931WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0418s vs `on_train_batch_end` time: 0.4507s). Check your callbacks.\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5580\n",
            "Epoch 00001: loss improved from inf to 0.55796, saving model to /content/drive/My Drive/fingerprint/model/model_test4.h5\n",
            "50/50 [==============================] - 30s 603ms/step - loss: 0.5580\n",
            "Epoch 2/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.4466\n",
            "Epoch 00002: loss improved from 0.55796 to 0.44657, saving model to /content/drive/My Drive/fingerprint/model/model_test4.h5\n",
            "50/50 [==============================] - 33s 655ms/step - loss: 0.4466\n",
            "Epoch 3/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3916\n",
            "Epoch 00003: loss improved from 0.44657 to 0.39160, saving model to /content/drive/My Drive/fingerprint/model/model_test4.h5\n",
            "50/50 [==============================] - 33s 662ms/step - loss: 0.3916\n",
            "Epoch 4/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3313\n",
            "Epoch 00004: loss improved from 0.39160 to 0.33134, saving model to /content/drive/My Drive/fingerprint/model/model_test4.h5\n",
            "50/50 [==============================] - 33s 662ms/step - loss: 0.3313\n",
            "Epoch 5/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2822\n",
            "Epoch 00005: loss improved from 0.33134 to 0.28224, saving model to /content/drive/My Drive/fingerprint/model/model_test4.h5\n",
            "50/50 [==============================] - 33s 663ms/step - loss: 0.2822\n",
            "Epoch 6/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2348\n",
            "Epoch 00006: loss improved from 0.28224 to 0.23477, saving model to /content/drive/My Drive/fingerprint/model/model_test4.h5\n",
            "50/50 [==============================] - 33s 661ms/step - loss: 0.2348\n",
            "Epoch 7/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.1856\n",
            "Epoch 00007: loss improved from 0.23477 to 0.18562, saving model to /content/drive/My Drive/fingerprint/model/model_test4.h5\n",
            "50/50 [==============================] - 33s 667ms/step - loss: 0.1856\n",
            "Epoch 8/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.1547\n",
            "Epoch 00008: loss improved from 0.18562 to 0.15473, saving model to /content/drive/My Drive/fingerprint/model/model_test4.h5\n",
            "50/50 [==============================] - 34s 674ms/step - loss: 0.1547\n",
            "Epoch 9/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.1400\n",
            "Epoch 00009: loss improved from 0.15473 to 0.13998, saving model to /content/drive/My Drive/fingerprint/model/model_test4.h5\n",
            "50/50 [==============================] - 33s 664ms/step - loss: 0.1400\n",
            "Epoch 10/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.1067\n",
            "Epoch 00010: loss improved from 0.13998 to 0.10665, saving model to /content/drive/My Drive/fingerprint/model/model_test4.h5\n",
            "50/50 [==============================] - 33s 662ms/step - loss: 0.1067\n",
            "Epoch 11/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0756\n",
            "Epoch 00011: loss improved from 0.10665 to 0.07560, saving model to /content/drive/My Drive/fingerprint/model/model_test4.h5\n",
            "50/50 [==============================] - 33s 664ms/step - loss: 0.0756\n",
            "Epoch 12/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0675\n",
            "Epoch 00012: loss improved from 0.07560 to 0.06746, saving model to /content/drive/My Drive/fingerprint/model/model_test4.h5\n",
            "50/50 [==============================] - 34s 681ms/step - loss: 0.0675\n",
            "Epoch 13/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0468\n",
            "Epoch 00013: loss improved from 0.06746 to 0.04682, saving model to /content/drive/My Drive/fingerprint/model/model_test4.h5\n",
            "50/50 [==============================] - 35s 694ms/step - loss: 0.0468\n",
            "Epoch 14/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0408\n",
            "Epoch 00014: loss improved from 0.04682 to 0.04080, saving model to /content/drive/My Drive/fingerprint/model/model_test4.h5\n",
            "50/50 [==============================] - 34s 676ms/step - loss: 0.0408\n",
            "Epoch 15/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0369\n",
            "Epoch 00015: loss improved from 0.04080 to 0.03694, saving model to /content/drive/My Drive/fingerprint/model/model_test4.h5\n",
            "50/50 [==============================] - 33s 663ms/step - loss: 0.0369\n",
            "Epoch 16/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0338\n",
            "Epoch 00016: loss improved from 0.03694 to 0.03377, saving model to /content/drive/My Drive/fingerprint/model/model_test4.h5\n",
            "50/50 [==============================] - 33s 670ms/step - loss: 0.0338\n",
            "Epoch 17/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0323\n",
            "Epoch 00017: loss improved from 0.03377 to 0.03226, saving model to /content/drive/My Drive/fingerprint/model/model_test4.h5\n",
            "50/50 [==============================] - 33s 664ms/step - loss: 0.0323\n",
            "Epoch 18/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0299\n",
            "Epoch 00018: loss improved from 0.03226 to 0.02995, saving model to /content/drive/My Drive/fingerprint/model/model_test4.h5\n",
            "50/50 [==============================] - 35s 694ms/step - loss: 0.0299\n",
            "Epoch 19/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0282\n",
            "Epoch 00019: loss improved from 0.02995 to 0.02820, saving model to /content/drive/My Drive/fingerprint/model/model_test4.h5\n",
            "50/50 [==============================] - 35s 694ms/step - loss: 0.0282\n",
            "Epoch 20/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0274\n",
            "Epoch 00020: loss improved from 0.02820 to 0.02740, saving model to /content/drive/My Drive/fingerprint/model/model_test4.h5\n",
            "50/50 [==============================] - 34s 679ms/step - loss: 0.0274\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 150528)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 150528)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential (Sequential)         (None, 1024)         107115840   input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 1024)         0           sequential[0][0]                 \n",
            "                                                                 sequential[1][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            1025        lambda[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 107,116,865\n",
            "Trainable params: 107,116,865\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Elapsed time acquired for 20 epoch(s) -> 11.31226752201716 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-u8hOzXJ3qb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "recognizer = Recognizer()\n",
        "recognizer.load_model('{}/model_test5.h5'.format(model_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRlBCCw6ZyKF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "c77e419e-7517-476c-caa4-5f09298f92a1"
      },
      "source": [
        "test_images = recognizer.prepare_images_from_dir(\"/content/drive/My Drive/fingerprint/test_images/\")\n",
        "class_images = \"/content/drive/My Drive/fingerprint/images/\"\n",
        "\n",
        "samples = {}\n",
        "\n",
        "for class_name in os.listdir(class_images):\n",
        "  samples[class_name] = recognizer.prepare_images_from_dir(class_images + class_name + \"/\")\n",
        "\n",
        "test_images_names = os.listdir(\"/content/drive/My Drive/fingerprint/test_images/\")\n",
        "\n",
        "i = 0\n",
        "threshold = 0.9\n",
        "\n",
        "for image in test_images:\n",
        "  image = image.reshape((1, -1))\n",
        "  for class_name in os.listdir(class_images):\n",
        "    for sample in samples[class_name][:2]:\n",
        "      # print(class_name)\n",
        "      sample = sample.reshape((1 ,-1))\n",
        "      prediction_score = recognizer.predict([image, sample])[0]\n",
        "      # print(prediction_score)\n",
        "      if prediction_score > threshold:\n",
        "          print( 'IMAGE {} is {} with confidence of {}'.format( test_images_names[i]  , class_name, prediction_score[0]) )\n",
        "          break\n",
        "    if prediction_score > threshold:\n",
        "      break\n",
        "  i += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IMAGE thumb.jpg is thumb with confidence of 0.9918048977851868\n",
            "IMAGE first.jpg is first with confidence of 0.9564779996871948\n",
            "IMAGE middle.jpg is middle with confidence of 0.9842307567596436\n",
            "IMAGE pinky.jpg is pinky with confidence of 0.9520330429077148\n",
            "IMAGE ring.jpg is ring with confidence of 0.9732805490493774\n",
            "IMAGE thumb_test.jpg is thumb with confidence of 0.9593909978866577\n",
            "IMAGE pinky_test.jpg is pinky with confidence of 0.9787534475326538\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qap9YFDB47ES",
        "colab_type": "text"
      },
      "source": [
        "## Adding a new peron's fingerprint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8IlZEBu_NUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# adding new fingerprint steps\n",
        "1. augment\n",
        "2. preprocess\n",
        "3. add to X1, X2 Y npy\n",
        "4. retrain model\n",
        "5. save model\n",
        "6. test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-je7iGz4tl6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# augment\n",
        "folder = \"/content/drive/My Drive/fingerprint/images/thumb_copy\"\n",
        "file_name = \"thumb.jpg\"\n",
        "augment(folder,file_name)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6614rz8a6du7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "0f7b39d6-5435-4284-d0b7-3eab39b567a8"
      },
      "source": [
        "#pre-process\n",
        "dimen = 224\n",
        "image_names = os.listdir(folder)\n",
        "\n",
        "images = []\n",
        "for image_path in image_names:\n",
        "  path = os.path.join(folder, image_path)\n",
        "  try:\n",
        "    print(path)\n",
        "    image = Image.open(path)\n",
        "    resize_image = image.resize((dimen, dimen))\n",
        "    array_ = list()\n",
        "    for x in range(dimen):\n",
        "      sub_array = list()\n",
        "      for y in range(dimen):\n",
        "        sub_array.append(resize_image.load()[x, y])\n",
        "      array_.append(sub_array)\n",
        "    image_data = np.array(array_)\n",
        "    image = np.array(np.reshape(image_data, (dimen, dimen, 3))) / 255\n",
        "    images.append(image)\n",
        "  except:\n",
        "      print('WARNING : File {} could not be processed.'.format(path))\n",
        "\n",
        "images = np.array(images)\n",
        "np.save('{0}/{1}_processed.npy'.format(folder,folder.split(\"/\")[-1]), images)\n",
        "print(\"Save Complete\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/fingerprint/images/thumb_copy/thumb.jpg\n",
            "/content/drive/My Drive/fingerprint/images/thumb_copy/thumb_flipped.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb_copy/thumb_rotated1.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb_copy/thumb_rotated2.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb_copy/thumb_cropped1.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb_copy/thumb_cropped2.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb_copy/thumb_bright1.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb_copy/thumb_bright2.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb_copy/thumb_blur1.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb_copy/thumb_blur2.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb_copy/thumb_copy_processed.npy\n",
            "WARNING : File /content/drive/My Drive/fingerprint/images/thumb_copy/thumb_copy_processed.npy could not be processed.\n",
            "Save Complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2I7Q_o_9nxe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "e18b3e25-bda0-452d-9b15-62d9cef2fd88"
      },
      "source": [
        "# add to X1, X2 Y npy\n",
        "saved_images = np.load('{}/images.npy'.format(out_path))\n",
        "print(\"Shape of saved images\")\n",
        "print(saved_images.shape)\n",
        "new_images = np.concatenate((saved_images,images))\n",
        "print(\"Shape of new images\")\n",
        "print(new_images.shape)\n",
        "np.save( '{}/images.npy'.format(out_path), new_images)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of saved images\n",
            "(50, 224, 224, 3)\n",
            "Shape of new images\n",
            "(60, 224, 224, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOmLQWWaKnPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samples_1 = []\n",
        "samples_2 = []\n",
        "labels = []\n",
        "\n",
        "for i in range(len(new_images)):\n",
        "  for j in range(len(new_images)):\n",
        "    samples_1.append(new_images[i])\n",
        "    samples_2.append(new_images[j])\n",
        "    t = i - i%10 +10\n",
        "    if t - 10 <= j < t:\n",
        "      labels.append(1)\n",
        "    else:\n",
        "      labels.append(0)\n",
        "\n",
        "X1 = np.array(samples_1)\n",
        "X2 = np.array(samples_2)\n",
        "Y = np.array(labels)\n",
        "\n",
        "np.save( '{}/x1.npy'.format( out_path ), X1 )\n",
        "np.save( '{}/x2.npy'.format( out_path ), X2 )\n",
        "np.save( '{}/y.npy'.format( out_path ) , Y )"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usdvKLkbL8DE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# retrain model\n",
        "use previous code\n",
        "# save model\n",
        "use previous code\n",
        "# test\n",
        "use previous code"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}