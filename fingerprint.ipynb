{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fingerprint.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1uw4dH2wKWmqX7Bic_TDtsPDiAB-ZNDRl",
      "authorship_tag": "ABX9TyOxh24FHK2PUPBgYnd6SNc7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utsav-195/fingerprint-recognition-using-siamese-network-with-retraining/blob/master/fingerprint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrqyAzxChL2T"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQiznhCQhp4K"
      },
      "source": [
        "# importing the required libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import imageio\n",
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa\n",
        "\n",
        "from tensorflow import keras\n",
        "from PIL import Image"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4sU-MHbCo1c"
      },
      "source": [
        "# function takes in the folder path and the filename to create synthetic images\n",
        "# using data augmentation techniques\n",
        "\n",
        "def augment(folder,file):\n",
        "  filename = folder + \"/\" + file\n",
        "  # loading in the images\n",
        "  image = imageio.imread(filename)\n",
        "\n",
        "  flip_vr=iaa.Flipud(p=1.0)\n",
        "  flip_vr_image= flip_vr.augment_image(image)\n",
        "\n",
        "  save_filename = folder + \"/\" + file.split(\".\")[0] + \"_flipped.png\"\n",
        "  cv2.imwrite(save_filename, flip_vr_image)\n",
        "\n",
        "  rotate = iaa.Affine(rotate=(50, -50))\n",
        "  rotated_image = rotate.augment_image(image)\n",
        "  save_filename = folder + \"/\" + file.split(\".\")[0] + \"_rotated1.png\"\n",
        "  cv2.imwrite(save_filename, rotated_image)\n",
        "\n",
        "  rotate = iaa.Affine(rotate=(50, -50))\n",
        "  rotated_image = rotate.augment_image(image)\n",
        "  save_filename = folder + \"/\" + file.split(\".\")[0] + \"_rotated2.png\"\n",
        "  cv2.imwrite(save_filename, rotated_image)\n",
        "\n",
        "  crop = iaa.Crop(percent=(0, 0.3)) # crop image\n",
        "  crop_image=crop.augment_image(image)\n",
        "  save_filename = folder + \"/\" + file.split(\".\")[0] + \"_cropped1.png\"\n",
        "  cv2.imwrite(save_filename, crop_image)\n",
        "\n",
        "  crop = iaa.Crop(percent=(0, 0.3)) # crop image\n",
        "  crop_image=crop.augment_image(image)\n",
        "  save_filename = folder + \"/\" + file.split(\".\")[0] + \"_cropped2.png\"\n",
        "  cv2.imwrite(save_filename, crop_image)\n",
        "\n",
        "  contrast=iaa.GammaContrast(gamma=2.0)\n",
        "  contrast_image =contrast.augment_image(image)\n",
        "  save_filename = folder + \"/\" + file.split(\".\")[0] + \"_bright1.png\"\n",
        "  cv2.imwrite(save_filename, contrast_image)\n",
        "\n",
        "  contrast=iaa.GammaContrast(gamma=1.4)\n",
        "  contrast_image =contrast.augment_image(image)\n",
        "  save_filename = folder + \"/\" + file.split(\".\")[0] + \"_bright2.png\"\n",
        "  cv2.imwrite(save_filename, contrast_image)\n",
        "\n",
        "  blur = iaa.GaussianBlur(sigma=4.0)\n",
        "  blur_image=blur.augment_image(image)\n",
        "  save_filename = folder + \"/\" + file.split(\".\")[0] + \"_blur1.png\"\n",
        "  cv2.imwrite(save_filename, blur_image)\n",
        "\n",
        "  blur = iaa.GaussianBlur(sigma=2.0)\n",
        "  blur_image=blur.augment_image(image)\n",
        "  save_filename = folder + \"/\" + file.split(\".\")[0] + \"_blur2.png\"\n",
        "  cv2.imwrite(save_filename, blur_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwtNBh1rjOsG"
      },
      "source": [
        "# setup image dimension and folder paths\n",
        "dimen = 224\n",
        "\n",
        "dir_path = \"/content/drive/My Drive/fingerprint/images/\"\n",
        "out_path = \"/content/drive/My Drive/fingerprint/processed_images/\"\n",
        "model_path = \"/content/drive/My Drive/fingerprint/model/\""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCpqIaxJ_zF7",
        "outputId": "08e79e39-9c6b-4871-c522-bed54ceda1da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Augment the image\n",
        "# Pre-process images to convert them into numpy arrays\n",
        "# store them in individual folders for future processing\n",
        "\n",
        "sub_dir_list = os.listdir(dir_path)\n",
        "\n",
        "images = []\n",
        "for i in range(len(sub_dir_list)):\n",
        "  image_names = os.listdir(os.path.join(dir_path, sub_dir_list[i]))\n",
        "  augment(os.path.join(dir_path, sub_dir_list[i]),image_names[0])\n",
        "  image_names = os.listdir(os.path.join(dir_path, sub_dir_list[i]))\n",
        "  sub_dir_images = []\n",
        "  for image_path in image_names:\n",
        "    path = os.path.join(dir_path, sub_dir_list[i], image_path )\n",
        "    try:\n",
        "      print(path)\n",
        "      image = Image.open(path)\n",
        "      resize_image = image.resize((dimen, dimen))\n",
        "      array_ = list()\n",
        "      for x in range(dimen):\n",
        "        sub_array = list()\n",
        "        for y in range(dimen):\n",
        "          sub_array.append(resize_image.load()[x, y])\n",
        "        array_.append(sub_array)\n",
        "      image_data = np.array(array_)\n",
        "      image = np.array(np.reshape(image_data, (dimen, dimen, 3))) / 255\n",
        "      sub_dir_images.append(image)\n",
        "      images.append(image)\n",
        "    except:\n",
        "      print('WARNING : File {} could not be processed.'.format(path))\n",
        "  sub_dir_images = np.array(sub_dir_images)\n",
        "  np.save( '{0}/{1}_processed.npy'.format(os.path.join(dir_path, sub_dir_list[i]),str(sub_dir_list[i])), sub_dir_images )\n",
        "  print(\"Save Complete\")\n",
        "\n",
        "images = np.array(images)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/fingerprint/images/thumb/thumb.jpg\n",
            "/content/drive/My Drive/fingerprint/images/thumb/thumb_flipped.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb/thumb_rotated1.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb/thumb_rotated2.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb/thumb_cropped1.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb/thumb_cropped2.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb/thumb_bright1.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb/thumb_bright2.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb/thumb_blur1.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb/thumb_blur2.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb/thumb_processed.npy\n",
            "WARNING : File /content/drive/My Drive/fingerprint/images/thumb/thumb_processed.npy could not be processed.\n",
            "Save Complete\n",
            "/content/drive/My Drive/fingerprint/images/first/first.jpg\n",
            "/content/drive/My Drive/fingerprint/images/first/first_flipped.png\n",
            "/content/drive/My Drive/fingerprint/images/first/first_rotated1.png\n",
            "/content/drive/My Drive/fingerprint/images/first/first_rotated2.png\n",
            "/content/drive/My Drive/fingerprint/images/first/first_cropped1.png\n",
            "/content/drive/My Drive/fingerprint/images/first/first_cropped2.png\n",
            "/content/drive/My Drive/fingerprint/images/first/first_bright1.png\n",
            "/content/drive/My Drive/fingerprint/images/first/first_bright2.png\n",
            "/content/drive/My Drive/fingerprint/images/first/first_blur1.png\n",
            "/content/drive/My Drive/fingerprint/images/first/first_blur2.png\n",
            "/content/drive/My Drive/fingerprint/images/first/first_processed.npy\n",
            "WARNING : File /content/drive/My Drive/fingerprint/images/first/first_processed.npy could not be processed.\n",
            "Save Complete\n",
            "/content/drive/My Drive/fingerprint/images/middle/middle.jpg\n",
            "/content/drive/My Drive/fingerprint/images/middle/middle_flipped.png\n",
            "/content/drive/My Drive/fingerprint/images/middle/middle_rotated1.png\n",
            "/content/drive/My Drive/fingerprint/images/middle/middle_rotated2.png\n",
            "/content/drive/My Drive/fingerprint/images/middle/middle_cropped1.png\n",
            "/content/drive/My Drive/fingerprint/images/middle/middle_cropped2.png\n",
            "/content/drive/My Drive/fingerprint/images/middle/middle_bright1.png\n",
            "/content/drive/My Drive/fingerprint/images/middle/middle_bright2.png\n",
            "/content/drive/My Drive/fingerprint/images/middle/middle_blur1.png\n",
            "/content/drive/My Drive/fingerprint/images/middle/middle_blur2.png\n",
            "/content/drive/My Drive/fingerprint/images/middle/middle_processed.npy\n",
            "WARNING : File /content/drive/My Drive/fingerprint/images/middle/middle_processed.npy could not be processed.\n",
            "Save Complete\n",
            "/content/drive/My Drive/fingerprint/images/ring/ring.jpg\n",
            "/content/drive/My Drive/fingerprint/images/ring/ring_flipped.png\n",
            "/content/drive/My Drive/fingerprint/images/ring/ring_rotated1.png\n",
            "/content/drive/My Drive/fingerprint/images/ring/ring_rotated2.png\n",
            "/content/drive/My Drive/fingerprint/images/ring/ring_cropped1.png\n",
            "/content/drive/My Drive/fingerprint/images/ring/ring_cropped2.png\n",
            "/content/drive/My Drive/fingerprint/images/ring/ring_bright1.png\n",
            "/content/drive/My Drive/fingerprint/images/ring/ring_bright2.png\n",
            "/content/drive/My Drive/fingerprint/images/ring/ring_blur1.png\n",
            "/content/drive/My Drive/fingerprint/images/ring/ring_blur2.png\n",
            "/content/drive/My Drive/fingerprint/images/ring/ring_processed.npy\n",
            "WARNING : File /content/drive/My Drive/fingerprint/images/ring/ring_processed.npy could not be processed.\n",
            "Save Complete\n",
            "/content/drive/My Drive/fingerprint/images/pinky/pinky.jpg\n",
            "/content/drive/My Drive/fingerprint/images/pinky/pinky_flipped.png\n",
            "/content/drive/My Drive/fingerprint/images/pinky/pinky_rotated1.png\n",
            "/content/drive/My Drive/fingerprint/images/pinky/pinky_rotated2.png\n",
            "/content/drive/My Drive/fingerprint/images/pinky/pinky_cropped1.png\n",
            "/content/drive/My Drive/fingerprint/images/pinky/pinky_cropped2.png\n",
            "/content/drive/My Drive/fingerprint/images/pinky/pinky_bright1.png\n",
            "/content/drive/My Drive/fingerprint/images/pinky/pinky_bright2.png\n",
            "/content/drive/My Drive/fingerprint/images/pinky/pinky_blur1.png\n",
            "/content/drive/My Drive/fingerprint/images/pinky/pinky_blur2.png\n",
            "/content/drive/My Drive/fingerprint/images/pinky/pinky_processed.npy\n",
            "WARNING : File /content/drive/My Drive/fingerprint/images/pinky/pinky_processed.npy could not be processed.\n",
            "Save Complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnU8l6rmf6WL"
      },
      "source": [
        "# Create image pairs and label them 1 if they belong to the same person, 0 otherwise\n",
        "samples_1 = []\n",
        "samples_2 = []\n",
        "labels = []\n",
        "\n",
        "for i in range(len(images)):\n",
        "  for j in range(len(images)):\n",
        "    samples_1.append(images[i])\n",
        "    samples_2.append(images[j])\n",
        "    t = i - i%10 +10\n",
        "    if t - 10 <= j < t:\n",
        "      labels.append(1)\n",
        "    else:\n",
        "      labels.append(0)\n",
        "\n",
        "X1 = np.array(samples_1)\n",
        "X2 = np.array(samples_2)\n",
        "Y = np.array(labels)\n",
        "\n",
        "np.save( '{}/images.npy'.format( out_path ), images )\n",
        "np.save( '{}/x1.npy'.format( out_path ), X1 )\n",
        "np.save( '{}/x2.npy'.format( out_path ), X2 )\n",
        "np.save( '{}/y.npy'.format( out_path ) , Y )"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iz-D8i3AFIG"
      },
      "source": [
        "# necessary imports\n",
        "from tensorflow.keras import models , optimizers , losses ,activations , callbacks\n",
        "from tensorflow.keras.layers import *\n",
        "import tensorflow.keras.backend as K\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Model class\n",
        "class Recognizer (object) :\n",
        "\n",
        "\t# initializes the model\n",
        "\tdef __init__( self ):\n",
        "\n",
        "\t\t# tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "\t\tself.__DIMEN = 224\n",
        "\n",
        "\t\tinput_shape = ((self.__DIMEN**2) * 3 , )\n",
        "\t\tconvolution_shape = (self.__DIMEN , self.__DIMEN , 3)\n",
        "\t\tkernel_size_1 = (10 ,10)\n",
        "\t\tkernel_size_2 = (7 ,7)\n",
        "\t\tkernel_size_3 = (4 ,4)\n",
        "\t\t# kernel_size_2 = ( 7 , 7 )\n",
        "\t\n",
        "\t\t# pool_size_1 = ( 3 , 3 )\n",
        "\t\t# pool_size_2 = ( 2 , 2 )\n",
        "\t\tstrides = 1\n",
        "\n",
        "\t\tseq_conv_model = [\n",
        "\n",
        "\t\t\tReshape( input_shape=input_shape , target_shape=convolution_shape),\n",
        "\n",
        "\t\t\tConv2D( 64, kernel_size=kernel_size_1 , strides=strides , activation='relu' ),\n",
        "\t\t\t# Conv2D( 32, kernel_size=kernel_size_1, strides=strides, activation='relu'),\n",
        "\t\t\tMaxPooling2D(strides=2),\n",
        "\n",
        "\t\t\tConv2D( 128, kernel_size=kernel_size_2 , strides=strides , activation='relu'),\n",
        "\t\t\t# Conv2D( 64, kernel_size=kernel_size_2 , strides=strides , activation='relu'),\n",
        "\t\t\tMaxPooling2D(strides=2),\n",
        "\n",
        "      Conv2D( 128, kernel_size=kernel_size_3 , strides=strides , activation='relu'),\n",
        "\t\t\t# Conv2D( 128, kernel_size=kernel_size_3 , strides=strides , activation='relu'),\n",
        "\t\t\tMaxPooling2D(strides=2),\n",
        "\n",
        "\t\t\tConv2D( 256, kernel_size=kernel_size_3 , strides=strides , activation='relu'),\n",
        "\t\t\t# Conv2D( 128, kernel_size=kernel_size_3 , strides=strides , activation='relu'),\n",
        "\n",
        "\t\t\tFlatten(),\n",
        "\n",
        "\t\t\tDense(1024, activation='relu'),\n",
        "\t \t\tDense(1024, activation=activations.sigmoid)\n",
        "\n",
        "\t\t]\n",
        "\n",
        "\t\tseq_model = tf.keras.Sequential(seq_conv_model)\n",
        "\n",
        "\t\tinput_x1 = Input(shape=input_shape)\n",
        "\t\tinput_x2 = Input(shape=input_shape)\n",
        "\n",
        "\t\toutput_x1 = seq_model(input_x1)\n",
        "\t\toutput_x2 = seq_model(input_x2)\n",
        "\n",
        "\t\tdistance_euclid = Lambda( lambda tensors : K.abs( tensors[0] - tensors[1] ))( [output_x1 , output_x2] )\n",
        "\t\toutputs = Dense( 1 , activation=activations.sigmoid) ( distance_euclid )\n",
        "\t\tself.__model = models.Model( [ input_x1 , input_x2 ] , outputs )\n",
        "\n",
        "\t\tself.__model.compile( loss=losses.binary_crossentropy , optimizer=optimizers.Adam(lr=0.00003))\n",
        "\n",
        "\n",
        "\t# trains the model\n",
        "\tdef fit(self, X, Y, hyperparameters):\n",
        "\t\tinitial_time = time.time()\n",
        "\t\t# for layer in self.__model.layers:\n",
        "\t\t# \tprint(layer.output_shape)\n",
        "\t\tself.__model.fit( X, Y,\n",
        "\t\t\t\t\t\t batch_size=hyperparameters[ 'batch_size' ] ,\n",
        "\t\t\t\t\t\t epochs=hyperparameters[ 'epochs' ] ,\n",
        "\t\t\t\t\t\t callbacks=hyperparameters[ 'callbacks'],\n",
        "\t\t\t\t\t\t validation_data=hyperparameters[ 'val_data' ]\n",
        "\t\t\t\t\t\t )\n",
        "\t\tfinal_time = time.time()\n",
        "\t\teta = (final_time - initial_time)\n",
        "\t\ttime_unit = 'seconds'\n",
        "\t\tif eta >= 60 :\n",
        "\t\t\teta = eta / 60\n",
        "\t\t\ttime_unit = 'minutes'\n",
        "\t\tself.__model.summary( )\n",
        "\t\tprint( 'Elapsed time acquired for {} epoch(s) -> {} {}'.format( hyperparameters[ 'epochs' ] , eta , time_unit ) )\n",
        "\n",
        "\t# converts the images in the folder to the required format\n",
        "\tdef prepare_images_from_dir( self , dir_path , flatten=True ):\n",
        "\t\timages_names = os.listdir(dir_path)\n",
        "\t\tf = [name for name in images_names if '.npy' in name]\n",
        "\t\tif len(f) == 0:\n",
        "\t\t\timages = list()\n",
        "\t\t\tfor imageName in images_names:\n",
        "\t\t\t\t# print(imageName)\n",
        "\t\t\t\timage = Image.open(dir_path + imageName)\n",
        "\t\t\t\tresize_image = image.resize((self.__DIMEN, self.__DIMEN))\n",
        "\t\t\t\tarray = list()\n",
        "\t\t\t\tfor x in range(self.__DIMEN):\n",
        "\t\t\t\t\tsub_array = list()\n",
        "\t\t\t\t\tfor y in range(self.__DIMEN):\n",
        "\t\t\t\t\t\tsub_array.append(resize_image.load()[x, y])\n",
        "\t\t\t\t\tarray.append(sub_array)\n",
        "\t\t\t\timage_data = np.array(array)\n",
        "\t\t\t\timage = np.array(np.reshape(image_data,(self.__DIMEN, self.__DIMEN, 3))) /255\n",
        "\t\t\t\timages.append(image)\n",
        "\n",
        "\t\t\tif flatten:\n",
        "\t\t\t\timages = np.array(images)\n",
        "\t\t\t\treturn images.reshape(images.shape[0], self.__DIMEN**2 * 3).astype(np.float32)\n",
        "\t\t\telse:\n",
        "\t\t\t\treturn np.array(images)\n",
        "\t\telse:\n",
        "\t\t\timages = np.load('{0}{1}'.format(dir_path,f[0]))\n",
        "\t\t\tif flatten:\n",
        "\t\t\t\timages = np.array(images)\n",
        "\t\t\t\treturn images.reshape((images.shape[0], self.__DIMEN**2 * 3)).astype(np.float32)\n",
        "\t\t\telse:\n",
        "\t\t\t\treturn np.array(images)\n",
        "\n",
        "\n",
        "\t# def evaluate(self, test_X, test_Y) :\n",
        "\t# \treturn self.__model.evaluate(test_X, test_Y)\n",
        "\n",
        "\t# gives the model prediction for the given input\n",
        "\tdef predict(self, X):\n",
        "\t\tpredictions = self.__model.predict(X)\n",
        "\t\treturn predictions\n",
        "\n",
        "\t# gives the model parameters\n",
        "\tdef summary(self):\n",
        "\t\tself.__model.summary()\n",
        "\n",
        "\t# saves the model to the specified filepath\n",
        "\tdef save_model(self, file_path):\n",
        "\t\tself.__model.save(file_path)\n",
        "\n",
        "\t# loads the model from the specified path\n",
        "\tdef load_model(self, file_path):\n",
        "\t\tself.__model = models.load_model(file_path)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLzRshWj8E0P"
      },
      "source": [
        "# instantiating the class\n",
        "recognizer = Recognizer()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuNmJTsZ8J05",
        "outputId": "c7d5d033-a84e-4efc-9f77-77e1cb909de8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "recognizer.summary()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 150528)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            [(None, 150528)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential_4 (Sequential)       (None, 1024)         107115840   input_5[0][0]                    \n",
            "                                                                 input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 1024)         0           sequential_4[0][0]               \n",
            "                                                                 sequential_4[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 1)            1025        lambda_2[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 107,116,865\n",
            "Trainable params: 107,116,865\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yoFU-1KcYF9"
      },
      "source": [
        "# load the saved numpy arrays back\n",
        "# do not execute this cell if you already have X1 and X2 created from previous execution\n",
        "import numpy as np\n",
        "\n",
        "data_dimension = 224\n",
        "\n",
        "X1 = np.load('{}/x1.npy'.format(out_path))\n",
        "X2 = np.load('{}/x2.npy'.format(out_path))\n",
        "Y = np.load('{}/y.npy'.format(out_path))\n",
        "\n",
        "X1 = X1.reshape((X1.shape[0], data_dimension**2 * 3)).astype(np.float32)\n",
        "X2 = X2.reshape((X2.shape[0], data_dimension**2 * 3)).astype(np.float32)\n",
        "\n",
        "print(X1.shape)\n",
        "print(X2.shape)\n",
        "print(Y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsDtTHOXAKeX",
        "outputId": "b626c8df-2b4e-4570-aa7d-ddecb26922e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# setting the model hyperparameters and training it\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# checkpointer to save the best model\n",
        "checkpointer = ModelCheckpoint(filepath='{}/model_test6.h5'.format(model_path), monitor='loss', verbose=1, save_best_only=True)\n",
        "\n",
        "parameters = {\n",
        "    'batch_size' : 50,\n",
        "    'epochs' : 30,\n",
        "    'callbacks' : checkpointer,\n",
        "    'val_data' : None\n",
        "}\n",
        "\n",
        "recognizer.fit( [ X1 , X2 ], Y, hyperparameters=parameters)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2500, 150528)\n",
            "(2500, 150528)\n",
            "(2500,)\n",
            "Epoch 1/30\n",
            " 2/50 [>.............................] - ETA: 12s - loss: 0.6912WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0365s vs `on_train_batch_end` time: 0.4494s). Check your callbacks.\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5606\n",
            "Epoch 00001: loss improved from inf to 0.56060, saving model to /content/drive/My Drive/fingerprint/model/model_test6.h5\n",
            "50/50 [==============================] - 30s 603ms/step - loss: 0.5606\n",
            "Epoch 2/30\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.4542\n",
            "Epoch 00002: loss improved from 0.56060 to 0.45416, saving model to /content/drive/My Drive/fingerprint/model/model_test6.h5\n",
            "50/50 [==============================] - 55s 1s/step - loss: 0.4542\n",
            "Epoch 3/30\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3828\n",
            "Epoch 00003: loss improved from 0.45416 to 0.38275, saving model to /content/drive/My Drive/fingerprint/model/model_test6.h5\n",
            "50/50 [==============================] - 55s 1s/step - loss: 0.3828\n",
            "Epoch 4/30\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2919\n",
            "Epoch 00004: loss improved from 0.38275 to 0.29186, saving model to /content/drive/My Drive/fingerprint/model/model_test6.h5\n",
            "50/50 [==============================] - 53s 1s/step - loss: 0.2919\n",
            "Epoch 5/30\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2443\n",
            "Epoch 00005: loss improved from 0.29186 to 0.24429, saving model to /content/drive/My Drive/fingerprint/model/model_test6.h5\n",
            "50/50 [==============================] - 56s 1s/step - loss: 0.2443\n",
            "Epoch 6/30\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.1984\n",
            "Epoch 00006: loss improved from 0.24429 to 0.19841, saving model to /content/drive/My Drive/fingerprint/model/model_test6.h5\n",
            "50/50 [==============================] - 33s 658ms/step - loss: 0.1984\n",
            "Epoch 7/30\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.1457\n",
            "Epoch 00007: loss improved from 0.19841 to 0.14569, saving model to /content/drive/My Drive/fingerprint/model/model_test6.h5\n",
            "50/50 [==============================] - 33s 660ms/step - loss: 0.1457\n",
            "Epoch 8/30\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.1064\n",
            "Epoch 00008: loss improved from 0.14569 to 0.10642, saving model to /content/drive/My Drive/fingerprint/model/model_test6.h5\n",
            "50/50 [==============================] - 32s 644ms/step - loss: 0.1064\n",
            "Epoch 9/30\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0798\n",
            "Epoch 00009: loss improved from 0.10642 to 0.07978, saving model to /content/drive/My Drive/fingerprint/model/model_test6.h5\n",
            "50/50 [==============================] - 33s 655ms/step - loss: 0.0798\n",
            "Epoch 10/30\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0608\n",
            "Epoch 00010: loss improved from 0.07978 to 0.06083, saving model to /content/drive/My Drive/fingerprint/model/model_test6.h5\n",
            "50/50 [==============================] - 51s 1s/step - loss: 0.0608\n",
            "Epoch 11/30\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0503\n",
            "Epoch 00011: loss improved from 0.06083 to 0.05027, saving model to /content/drive/My Drive/fingerprint/model/model_test6.h5\n",
            "50/50 [==============================] - 33s 665ms/step - loss: 0.0503\n",
            "Epoch 12/30\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0423\n",
            "Epoch 00012: loss improved from 0.05027 to 0.04233, saving model to /content/drive/My Drive/fingerprint/model/model_test6.h5\n",
            "50/50 [==============================] - 54s 1s/step - loss: 0.0423\n",
            "Epoch 13/30\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0377\n",
            "Epoch 00013: loss improved from 0.04233 to 0.03774, saving model to /content/drive/My Drive/fingerprint/model/model_test6.h5\n",
            "50/50 [==============================] - 33s 657ms/step - loss: 0.0377\n",
            "Epoch 14/30\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0339\n",
            "Epoch 00014: loss improved from 0.03774 to 0.03388, saving model to /content/drive/My Drive/fingerprint/model/model_test6.h5\n",
            "50/50 [==============================] - 33s 666ms/step - loss: 0.0339\n",
            "Epoch 15/30\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0310\n",
            "Epoch 00015: loss improved from 0.03388 to 0.03105, saving model to /content/drive/My Drive/fingerprint/model/model_test6.h5\n",
            "50/50 [==============================] - 34s 674ms/step - loss: 0.0310\n",
            "Epoch 16/30\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0295\n",
            "Epoch 00016: loss improved from 0.03105 to 0.02949, saving model to /content/drive/My Drive/fingerprint/model/model_test6.h5\n",
            "50/50 [==============================] - 53s 1s/step - loss: 0.0295\n",
            "Epoch 17/30\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0273\n",
            "Epoch 00017: loss improved from 0.02949 to 0.02734, saving model to /content/drive/My Drive/fingerprint/model/model_test6.h5\n",
            "50/50 [==============================] - 34s 685ms/step - loss: 0.0273\n",
            "Epoch 18/30\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0257\n",
            "Epoch 00018: loss improved from 0.02734 to 0.02568, saving model to /content/drive/My Drive/fingerprint/model/model_test6.h5\n",
            "50/50 [==============================] - 33s 668ms/step - loss: 0.0257\n",
            "Epoch 19/30\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0246\n",
            "Epoch 00019: loss improved from 0.02568 to 0.02463, saving model to /content/drive/My Drive/fingerprint/model/model_test6.h5\n",
            "50/50 [==============================] - 52s 1s/step - loss: 0.0246\n",
            "Epoch 20/30\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0234\n",
            "Epoch 00020: loss improved from 0.02463 to 0.02341, saving model to /content/drive/My Drive/fingerprint/model/model_test6.h5\n",
            "50/50 [==============================] - 32s 649ms/step - loss: 0.0234\n",
            "Epoch 21/30\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0227\n",
            "Epoch 00021: loss improved from 0.02341 to 0.02270, saving model to /content/drive/My Drive/fingerprint/model/model_test6.h5\n",
            "50/50 [==============================] - 34s 682ms/step - loss: 0.0227\n",
            "Epoch 22/30\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0222\n",
            "Epoch 00022: loss improved from 0.02270 to 0.02218, saving model to /content/drive/My Drive/fingerprint/model/model_test6.h5\n",
            "50/50 [==============================] - 33s 656ms/step - loss: 0.0222\n",
            "Epoch 23/30\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0215\n",
            "Epoch 00023: loss improved from 0.02218 to 0.02153, saving model to /content/drive/My Drive/fingerprint/model/model_test6.h5\n",
            "50/50 [==============================] - 52s 1s/step - loss: 0.0215\n",
            "Epoch 24/30\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0209\n",
            "Epoch 00024: loss improved from 0.02153 to 0.02093, saving model to /content/drive/My Drive/fingerprint/model/model_test6.h5\n",
            "50/50 [==============================] - 33s 657ms/step - loss: 0.0209\n",
            "Epoch 25/30\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0208\n",
            "Epoch 00025: loss improved from 0.02093 to 0.02079, saving model to /content/drive/My Drive/fingerprint/model/model_test6.h5\n",
            "50/50 [==============================] - 33s 651ms/step - loss: 0.0208\n",
            "Epoch 26/30\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0204\n",
            "Epoch 00026: loss improved from 0.02079 to 0.02036, saving model to /content/drive/My Drive/fingerprint/model/model_test6.h5\n",
            "50/50 [==============================] - 35s 696ms/step - loss: 0.0204\n",
            "Epoch 27/30\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0200\n",
            "Epoch 00027: loss improved from 0.02036 to 0.02000, saving model to /content/drive/My Drive/fingerprint/model/model_test6.h5\n",
            "50/50 [==============================] - 33s 658ms/step - loss: 0.0200\n",
            "Epoch 28/30\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0198\n",
            "Epoch 00028: loss improved from 0.02000 to 0.01979, saving model to /content/drive/My Drive/fingerprint/model/model_test6.h5\n",
            "50/50 [==============================] - 32s 650ms/step - loss: 0.0198\n",
            "Epoch 29/30\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0194\n",
            "Epoch 00029: loss improved from 0.01979 to 0.01936, saving model to /content/drive/My Drive/fingerprint/model/model_test6.h5\n",
            "50/50 [==============================] - 33s 657ms/step - loss: 0.0194\n",
            "Epoch 30/30\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0191\n",
            "Epoch 00030: loss improved from 0.01936 to 0.01906, saving model to /content/drive/My Drive/fingerprint/model/model_test6.h5\n",
            "50/50 [==============================] - 33s 657ms/step - loss: 0.0191\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 150528)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 150528)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential (Sequential)         (None, 1024)         107115840   input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 1024)         0           sequential[0][0]                 \n",
            "                                                                 sequential[1][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            1025        lambda[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 107,116,865\n",
            "Trainable params: 107,116,865\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Elapsed time acquired for 30 epoch(s) -> 19.7501326918602 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-u8hOzXJ3qb"
      },
      "source": [
        "# loading the saved model\n",
        "# do not execute this cell if you just trained the model\n",
        "recognizer = Recognizer()\n",
        "recognizer.load_model('{}/model_test6.h5'.format(model_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPCZisyudTIk"
      },
      "source": [
        "# preparing the test images and class images for prediction\n",
        "test_images = recognizer.prepare_images_from_dir(\"/content/drive/My Drive/fingerprint/test_images/\")\n",
        "class_images = \"/content/drive/My Drive/fingerprint/images/\"\n",
        "\n",
        "samples = {}\n",
        "\n",
        "for class_name in os.listdir(class_images):\n",
        "  samples[class_name] = recognizer.prepare_images_from_dir(class_images + class_name + \"/\")\n",
        "\n",
        "test_images_names = os.listdir(\"/content/drive/My Drive/fingerprint/test_images/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRlBCCw6ZyKF",
        "outputId": "c77e419e-7517-476c-caa4-5f09298f92a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "# setting a threshold of 0.9 for prediction confidence\n",
        "i = 0\n",
        "threshold = 0.9\n",
        "\n",
        "for image in test_images:\n",
        "  image = image.reshape((1, -1))\n",
        "  for class_name in os.listdir(class_images):\n",
        "    for sample in samples[class_name][:2]:\n",
        "      # print(class_name)\n",
        "      sample = sample.reshape((1 ,-1))\n",
        "      prediction_score = recognizer.predict([image, sample])[0]\n",
        "      # print(prediction_score)\n",
        "      if prediction_score > threshold:\n",
        "          print( 'IMAGE {} is {} with confidence of {}'.format( test_images_names[i]  , class_name, prediction_score[0]) )\n",
        "          break\n",
        "    if prediction_score > threshold:\n",
        "      break\n",
        "  i += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IMAGE thumb.jpg is thumb with confidence of 0.9918048977851868\n",
            "IMAGE first.jpg is first with confidence of 0.9564779996871948\n",
            "IMAGE middle.jpg is middle with confidence of 0.9842307567596436\n",
            "IMAGE pinky.jpg is pinky with confidence of 0.9520330429077148\n",
            "IMAGE ring.jpg is ring with confidence of 0.9732805490493774\n",
            "IMAGE thumb_test.jpg is thumb with confidence of 0.9593909978866577\n",
            "IMAGE pinky_test.jpg is pinky with confidence of 0.9787534475326538\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qap9YFDB47ES"
      },
      "source": [
        "## Adding a new peron's fingerprint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8IlZEBu_NUk"
      },
      "source": [
        "# adding new fingerprint steps\n",
        "1. augment\n",
        "2. preprocess\n",
        "3. add to X1, X2 Y npy\n",
        "4. retrain model\n",
        "5. save model\n",
        "6. test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-je7iGz4tl6"
      },
      "source": [
        "# augment\n",
        "folder = \"/content/drive/My Drive/fingerprint/images/thumb_copy\"\n",
        "file_name = \"thumb.jpg\"\n",
        "augment(folder,file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6614rz8a6du7",
        "outputId": "0f7b39d6-5435-4284-d0b7-3eab39b567a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "#pre-process\n",
        "dimen = 224\n",
        "image_names = os.listdir(folder)\n",
        "\n",
        "images = []\n",
        "for image_path in image_names:\n",
        "  path = os.path.join(folder, image_path)\n",
        "  try:\n",
        "    print(path)\n",
        "    image = Image.open(path)\n",
        "    resize_image = image.resize((dimen, dimen))\n",
        "    array_ = list()\n",
        "    for x in range(dimen):\n",
        "      sub_array = list()\n",
        "      for y in range(dimen):\n",
        "        sub_array.append(resize_image.load()[x, y])\n",
        "      array_.append(sub_array)\n",
        "    image_data = np.array(array_)\n",
        "    image = np.array(np.reshape(image_data, (dimen, dimen, 3))) / 255\n",
        "    images.append(image)\n",
        "  except:\n",
        "      print('WARNING : File {} could not be processed.'.format(path))\n",
        "\n",
        "images = np.array(images)\n",
        "np.save('{0}/{1}_processed.npy'.format(folder,folder.split(\"/\")[-1]), images)\n",
        "print(\"Save Complete\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/fingerprint/images/thumb_copy/thumb.jpg\n",
            "/content/drive/My Drive/fingerprint/images/thumb_copy/thumb_flipped.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb_copy/thumb_rotated1.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb_copy/thumb_rotated2.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb_copy/thumb_cropped1.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb_copy/thumb_cropped2.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb_copy/thumb_bright1.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb_copy/thumb_bright2.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb_copy/thumb_blur1.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb_copy/thumb_blur2.png\n",
            "/content/drive/My Drive/fingerprint/images/thumb_copy/thumb_copy_processed.npy\n",
            "WARNING : File /content/drive/My Drive/fingerprint/images/thumb_copy/thumb_copy_processed.npy could not be processed.\n",
            "Save Complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2I7Q_o_9nxe",
        "outputId": "e18b3e25-bda0-452d-9b15-62d9cef2fd88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# add to X1, X2 Y npy\n",
        "saved_images = np.load('{}/images.npy'.format(out_path))\n",
        "print(\"Shape of saved images\")\n",
        "print(saved_images.shape)\n",
        "new_images = np.concatenate((saved_images,images))\n",
        "print(\"Shape of new images\")\n",
        "print(new_images.shape)\n",
        "np.save( '{}/images.npy'.format(out_path), new_images)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of saved images\n",
            "(50, 224, 224, 3)\n",
            "Shape of new images\n",
            "(60, 224, 224, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOmLQWWaKnPe"
      },
      "source": [
        "# Create image pairs and label them 1 if they belong to the same person, 0 otherwise\n",
        "samples_1 = []\n",
        "samples_2 = []\n",
        "labels = []\n",
        "\n",
        "for i in range(len(new_images)):\n",
        "  for j in range(len(new_images)):\n",
        "    samples_1.append(new_images[i])\n",
        "    samples_2.append(new_images[j])\n",
        "    t = i - i%10 +10\n",
        "    if t - 10 <= j < t:\n",
        "      labels.append(1)\n",
        "    else:\n",
        "      labels.append(0)\n",
        "\n",
        "X1 = np.array(samples_1)\n",
        "X2 = np.array(samples_2)\n",
        "Y = np.array(labels)\n",
        "\n",
        "np.save( '{}/x1.npy'.format( out_path ), X1 )\n",
        "np.save( '{}/x2.npy'.format( out_path ), X2 )\n",
        "np.save( '{}/y.npy'.format( out_path ) , Y )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usdvKLkbL8DE"
      },
      "source": [
        "# retrain model\n",
        "use previous code\n",
        "# save model\n",
        "use previous code\n",
        "# test\n",
        "use previous code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPhyPbj2PIQD"
      },
      "source": [
        "# Pytorch Version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gd7CnDjlPHuu"
      },
      "source": [
        "# PyTorch libraries and modules\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout, Flatten\n",
        "from torch.optim import Adam, SGD\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import BCELoss"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZ-hNm2gMAlr"
      },
      "source": [
        "# setup image dimension and folder paths\n",
        "dimen = 224\n",
        "\n",
        "dir_path = \"/content/drive/My Drive/fingerprint/images/\"\n",
        "out_path = \"/content/drive/My Drive/fingerprint/processed_images/\"\n",
        "model_path = \"/content/drive/My Drive/fingerprint/model/\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rm2Ob68PdIO"
      },
      "source": [
        "# creating the model class\n",
        "class Siamese(nn.Module):\n",
        "\n",
        "    # initializing the model\n",
        "    def __init__(self):\n",
        "        super(Siamese, self).__init__()\n",
        "        self.dimen = 224\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, (10,10), stride=1), \n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
        "            nn.Conv2d(64, 128, (7,7), stride=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
        "            nn.Conv2d(128, 256, (5,5), stride=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
        "            nn.Conv2d(256, 384, (3,3)),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.linear = nn.Sequential(nn.Linear(169344, 2048), nn.ReLU(), nn.Linear(2048, 1024), nn.Sigmoid())\n",
        "        self.out = nn.Sequential(nn.Linear(1024, 1),nn.Sigmoid())\n",
        "\n",
        "    # passing image into the model to get output\n",
        "    def forward_one(self, x):\n",
        "        # for layer in self.conv:\n",
        "        #   x = layer(x.float())\n",
        "        #   print(x.size())\n",
        "        x = self.conv(x.float())\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        # print(x.size())\n",
        "        # for layer in self.linear:\n",
        "        #   x = layer(x.float())\n",
        "        #   print(x.size())\n",
        "        x = self.linear(x.float())\n",
        "        return x\n",
        "\n",
        "    # passing image to get model output\n",
        "    def forward(self, x1, x2):\n",
        "        out1 = self.forward_one(x1)\n",
        "        out2 = self.forward_one(x2)\n",
        "        dis = torch.abs(out1 - out2)\n",
        "        out = self.out(dis)\n",
        "        # print(out)\n",
        "        return out\n",
        "\n",
        "    # converts the images in the folder to the required format\n",
        "    def prepare_images_from_dir( self , dir_path , flatten=True ):\n",
        "      images_names = os.listdir(dir_path)\n",
        "      f = [name for name in images_names if '.npy' in name]\n",
        "      if len(f) == 0:\n",
        "        images = list()\n",
        "        for imageName in images_names:\n",
        "          # print(imageName)\n",
        "          image = Image.open(dir_path + imageName)\n",
        "          resize_image = image.resize((self.dimen, self.dimen))\n",
        "          array = list()\n",
        "          for x in range(self.dimen):\n",
        "            sub_array = list()\n",
        "            for y in range(self.dimen):\n",
        "              sub_array.append(resize_image.load()[x, y])\n",
        "            array.append(sub_array)\n",
        "          image_data = np.array(array)\n",
        "          image = np.array(np.reshape(image_data,(3, self.dimen, self.dimen))) /255\n",
        "          images.append(image)\n",
        "\n",
        "        if flatten:\n",
        "          images = np.array(images)\n",
        "          return images.reshape(images.shape[0], self.dimen**2 * 3).astype(np.float32)\n",
        "        else:\n",
        "          return torch.from_numpy(np.array(images))\n",
        "      else:\n",
        "        images = np.load('{0}{1}'.format(dir_path,f[0]))\n",
        "        if flatten:\n",
        "          images = np.array(images)\n",
        "          return images.reshape((images.shape[0], self.dimen**2 * 3)).astype(np.float32)\n",
        "        else:\n",
        "          images = np.array(np.reshape(images,(len(images), 3, self.dimen, self.dimen)))\n",
        "          return torch.from_numpy(images)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1zlUHRz_PNQ"
      },
      "source": [
        "# instantiating the class\n",
        "net = Siamese()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f3rYv-XLhJ1"
      },
      "source": [
        "# setting optimizer and loss function\n",
        "optimizer = Adam(net.parameters(), lr=0.00003)\n",
        "criterion = BCELoss()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSo5Pi8sLIq9"
      },
      "source": [
        "# loading the pre-processed data\n",
        "X1 = np.load('{}/x1.npy'.format(out_path))\n",
        "X2 = np.load('{}/x2.npy'.format(out_path))\n",
        "Y = np.load('{}/y.npy'.format(out_path))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ym4flAI6Tq6A",
        "outputId": "e492e6c0-307d-4009-8d05-c7ab4e9e5478",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X1.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2500, 224, 224, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWDGGCkiPuxy"
      },
      "source": [
        "# PyTorch expects data in the [Channels, Length, Width]\n",
        "# data store in [Length, Width, Channels] format. So, reshaping.\n",
        "X1 = X1.reshape(len(X1), 3, 224, 224)\n",
        "X2 = X2.reshape(len(X2), 3, 224, 224)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk8k_psLLSUJ"
      },
      "source": [
        "# converting to pytorch tensors\n",
        "X1  = torch.from_numpy(X1)\n",
        "X2  = torch.from_numpy(X2)\n",
        "Y  = torch.from_numpy(Y)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEvM6LZ5NZbB"
      },
      "source": [
        "# if GPU is available, then put the model into the GPU for faster processing\n",
        "if torch.cuda.is_available():\n",
        "  net = net.cuda()\n",
        "  criterion = criterion.cuda()\n",
        "  # X1 = X1.cuda()\n",
        "  # X2 = X2.cuda()\n",
        "  # Y = Y.cuda()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcfZEErTbq4z",
        "outputId": "fa4be5ea-47e6-45f4-c7f7-9faae436e37a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "source": [
        "print(net)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Siamese(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(10, 10), stride=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (9): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (10): ReLU(inplace=True)\n",
            "  )\n",
            "  (linear): Sequential(\n",
            "    (0): Linear(in_features=169344, out_features=2048, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "    (3): Sigmoid()\n",
            "  )\n",
            "  (out): Sequential(\n",
            "    (0): Linear(in_features=1024, out_features=1, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcuJPKg7OuFd"
      },
      "source": [
        "References:\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2019/10/building-image-classification-models-cnn-pytorch/\n",
        "\n",
        "\n",
        "https://github.com/fangpin/siamese-pytorch/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6sUvupVMXDo"
      },
      "source": [
        "# train the model\n",
        "def train(epoch,batch_size):\n",
        "    net.train()\n",
        "    tr_loss = 0\n",
        "\n",
        "    # clearing the Gradients of the model parameters\n",
        "    loss_train = 1\n",
        "    for i in tqdm(range(0,len(X1),batch_size)):\n",
        "      # print(\"i:\",i)\n",
        "      optimizer.zero_grad()\n",
        "      # x1, x2 = Variable(X1[i:i+batch_size]), Variable(X2[i:i+batch_size])\n",
        "\n",
        "      # load the data batch wise\n",
        "      x1, x2 = X1[i:i+batch_size], X2[i:i+batch_size]\n",
        "      y = Y[i:i+batch_size]\n",
        "      y = y.reshape(len(y),1)\n",
        "      \n",
        "      # if GPU available, put data into GPU for faster processing\n",
        "      # only adding batches into GPU to save GPU RAM\n",
        "      if torch.cuda.is_available():\n",
        "        x1,x2 = x1.cuda(),x2.cuda()\n",
        "        y=y.cuda()\n",
        "\n",
        "      # get model output\n",
        "      output = net.forward(x1, x2)\n",
        "      # calculate loss\n",
        "      loss_train = criterion(output.float(), y.float())\n",
        "      train_losses.append(loss_train)\n",
        "      # backpropagate\n",
        "      loss_train.backward()\n",
        "      optimizer.step()\n",
        "      tr_loss = loss_train.item()\n",
        "    print('Epoch : ',epoch+1, '\\t', 'loss :', loss_train)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGYiSrSFPjFO",
        "outputId": "9bd5b115-8995-46c4-ef25-3915c6dde26a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# defining the number of epochs\n",
        "n_epochs = 50\n",
        "# empty list to store training losses\n",
        "train_losses = []\n",
        "batch_size = 25\n",
        "# empty list to store validation losses\n",
        "\n",
        "# training the model\n",
        "for epoch in range(n_epochs):\n",
        "    train(epoch,batch_size)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  1 \t loss : tensor(0.6462, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  2 \t loss : tensor(0.5783, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:54<00:00,  1.83it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  3 \t loss : tensor(0.5816, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:55<00:00,  1.80it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  4 \t loss : tensor(0.4661, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:55<00:00,  1.79it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  5 \t loss : tensor(0.3943, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.78it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  6 \t loss : tensor(0.4973, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.78it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  7 \t loss : tensor(0.3487, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.77it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  8 \t loss : tensor(0.3295, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.76it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  9 \t loss : tensor(0.3135, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.76it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  10 \t loss : tensor(0.3190, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.76it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  11 \t loss : tensor(0.3283, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.76it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  12 \t loss : tensor(0.3644, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.76it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  13 \t loss : tensor(0.2726, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.76it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  14 \t loss : tensor(0.3272, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.76it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  15 \t loss : tensor(0.1962, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.76it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  16 \t loss : tensor(0.1803, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.76it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  17 \t loss : tensor(0.2999, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:57<00:00,  1.75it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  18 \t loss : tensor(0.2306, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.76it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  19 \t loss : tensor(0.2056, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:57<00:00,  1.75it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  20 \t loss : tensor(0.1895, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.76it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  21 \t loss : tensor(0.1773, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:57<00:00,  1.75it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  22 \t loss : tensor(0.1638, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:57<00:00,  1.75it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  23 \t loss : tensor(0.1558, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.76it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  24 \t loss : tensor(0.1469, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.76it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  25 \t loss : tensor(0.1401, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.76it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  26 \t loss : tensor(0.1329, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.76it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  27 \t loss : tensor(0.1251, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.76it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  28 \t loss : tensor(0.1173, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.76it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  29 \t loss : tensor(0.1092, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:57<00:00,  1.75it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  30 \t loss : tensor(0.1029, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.76it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  31 \t loss : tensor(0.0985, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.75it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  32 \t loss : tensor(0.0980, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.76it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  33 \t loss : tensor(0.0960, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.75it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  34 \t loss : tensor(0.0923, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:57<00:00,  1.75it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  35 \t loss : tensor(0.0903, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.76it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  36 \t loss : tensor(0.0895, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:57<00:00,  1.75it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  37 \t loss : tensor(0.0824, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.75it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  38 \t loss : tensor(0.0823, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.76it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  39 \t loss : tensor(0.0818, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:57<00:00,  1.75it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  40 \t loss : tensor(0.0796, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:57<00:00,  1.75it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  41 \t loss : tensor(0.0771, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.76it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  42 \t loss : tensor(0.0769, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.76it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  43 \t loss : tensor(0.0740, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:57<00:00,  1.75it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  44 \t loss : tensor(0.0735, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:57<00:00,  1.75it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  45 \t loss : tensor(0.0721, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.76it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  46 \t loss : tensor(0.0711, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:57<00:00,  1.75it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  47 \t loss : tensor(0.0685, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:56<00:00,  1.76it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  48 \t loss : tensor(0.0686, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:57<00:00,  1.75it/s]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  49 \t loss : tensor(0.0686, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:57<00:00,  1.75it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch :  50 \t loss : tensor(0.0689, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv6wMQYpx7Tr"
      },
      "source": [
        "# transferring X1,X2 to GPU to free up CPU\n",
        "# save model uses CPU to create the weights matrix\n",
        "X1 = X1.cuda()\n",
        "X2 = X2.cuda()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dduwhC9RICmb"
      },
      "source": [
        "# save PyTorch model weights\n",
        "torch.save(net.state_dict(), \"/content/drive/My Drive/fingerprint/model/torch2.model\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7CahpU7hKxL"
      },
      "source": [
        "# Load saved model\n",
        "# do not execute if model already present\n",
        "net = Siamese()\n",
        "net.load_state_dict(torch.load(\"/content/drive/My Drive/fingerprint/model/torch2.model\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aenEdmcIQlbT"
      },
      "source": [
        "# put the model in CPU if the test images are loaded in the CPU\n",
        "net = net.cpu()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyKdLO7chP8f"
      },
      "source": [
        "# preparing the test images and class images for prediction\n",
        "test_images = net.prepare_images_from_dir(\"/content/drive/My Drive/fingerprint/test_images/\",flatten=False)\n",
        "class_images = \"/content/drive/My Drive/fingerprint/images/\"\n",
        "\n",
        "samples = {}\n",
        "\n",
        "for class_name in os.listdir(class_images):\n",
        "  samples[class_name] = net.prepare_images_from_dir(class_images + class_name + \"/\",flatten=False)\n",
        "\n",
        "test_images_names = os.listdir(\"/content/drive/My Drive/fingerprint/test_images/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCrZh8aNG4QC",
        "outputId": "277c0387-ef4a-4e15-db67-39d120b08562",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "# setting a threshold of 0.9 for prediction confidence\n",
        "i = 0\n",
        "threshold = 0.9\n",
        "\n",
        "for j in range(0,len(test_images)):\n",
        "  for class_name in os.listdir(class_images):\n",
        "    for sample in range(len(samples[class_name])):\n",
        "      # print(class_name)\n",
        "      # sample = sample.reshape((1 ,-1))\n",
        "      prediction_score = net.forward(test_images[j:j+1], samples[class_name][sample:sample+1])[0]\n",
        "      # prediction_score = recognizer.predict([image, sample])[0]\n",
        "      # print(prediction_score)\n",
        "      if prediction_score > threshold:\n",
        "          print( 'IMAGE {} is {} with confidence of {}'.format( test_images_names[i]  , class_name, prediction_score[0]) )\n",
        "          break\n",
        "    if prediction_score > threshold:\n",
        "      break\n",
        "  i += 1"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IMAGE thumb.jpg is thumb with confidence of 0.9956912398338318\n",
            "IMAGE first.jpg is first with confidence of 0.9828104972839355\n",
            "IMAGE middle.jpg is middle with confidence of 0.9826158285140991\n",
            "IMAGE pinky.jpg is pinky with confidence of 0.9913548827171326\n",
            "IMAGE ring.jpg is ring with confidence of 0.9882948398590088\n",
            "IMAGE thumb_test.jpg is ring with confidence of 0.9845402836799622\n",
            "IMAGE pinky_test.jpg is ring with confidence of 0.9670038819313049\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvw8JR0ZiR2-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}